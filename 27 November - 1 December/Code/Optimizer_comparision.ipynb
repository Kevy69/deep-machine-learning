{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c82e207-9875-4617-8046-8aa0f4f01d45",
   "metadata": {},
   "source": [
    "# Optimizer Comparison Exercise\n",
    "## Objective\n",
    "Explore how different optimization algorithms affect the training of neural networks.\n",
    "\n",
    "## Background\n",
    "Optimizers are algorithms or methods used to change the attributes of the neural network, such as weights and learning rate, to reduce the losses. Common optimizers include SGD (Stochastic Gradient Descent), Mini-batch SGD, Momentum, Adam, and RMSProp.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b17beb-e01c-417a-a14e-56e70adf0bd6",
   "metadata": {},
   "source": [
    "# Part 1: Classification task\n",
    "\n",
    "## Setup\n",
    "Start by importing necessary libraries and preparing a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cb463e0-5770-4e6d-a618-3b815386c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create a dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 20)\n",
      "(800,)\n",
      "(200, 20)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create a dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954232b6-ab88-4616-94c8-808c931b9224",
   "metadata": {},
   "source": [
    "## Building the Neural Network\n",
    "Define a function to create a basic neural network. This function will take an optimizer as an argument. Define the network yourself with 2 hidden layers using 64 nodes each. They should both use relu as activation function. Dont forget the output layer which, for a binary classification problem should use what activation function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21b4afa7-446c-445f-abc9-597ecc04920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(optimizer):\n",
    "    # Add your model here following the above instructions.\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8c115-2569-4e3b-a986-dca054cfdcfb",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "Experiment with different optimizers. For each optimizer, train the model and plot its accuracy and loss.\n",
    "\n",
    "### Task:\n",
    "1. Try the following optimizers: 'sgd', 'adam', 'rmsprop', and a custom SGD with Momentum.\n",
    "2. For the custom SGD with Momentum, use: tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "3. Train each model for a fixed number of epochs (e.g., 30) and evaluate its performance on the test set.\n",
    "4. Do step 1-3 same, but change batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "125645a2-f848-44a5-af82-0d1b8032c172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with optimizer: sgd\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\losses.py\", line 2156, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\backend.py\", line 5707, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((32, 2) vs (32, 1)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kevin\\Documents\\School\\Code\\deep-machine-learning\\27 November - 1 December\\Code\\Optimizer_comparision.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Documents/School/Code/deep-machine-learning/27%20November%20-%201%20December/Code/Optimizer_comparision.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     model \u001b[39m=\u001b[39m build_model(opt)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Documents/School/Code/deep-machine-learning/27%20November%20-%201%20December/Code/Optimizer_comparision.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# We manually specify the validation data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Documents/School/Code/deep-machine-learning/27%20November%20-%201%20December/Code/Optimizer_comparision.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test), verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kevin/Documents/School/Code/deep-machine-learning/27%20November%20-%201%20December/Code/Optimizer_comparision.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m histories[\u001b[39mstr\u001b[39m(opt)] \u001b[39m=\u001b[39m history\n",
      "File \u001b[1;32mc:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file6ojnm0q3.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\losses.py\", line 2156, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\backend.py\", line 5707, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((32, 2) vs (32, 1)).\n"
     ]
    }
   ],
   "source": [
    "optimizers = ['sgd', 'adam', 'rmsprop', 'SGD_momentum']\n",
    "histories = {}\n",
    "\n",
    "for opt in optimizers:\n",
    "    print(f\"Training with optimizer: {opt}\")\n",
    "    \n",
    "    if opt == \"SGD_momentum\": \n",
    "        model = build_model(tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9))\n",
    "    else:\n",
    "        model = build_model(opt)\n",
    "    \n",
    "    # We manually specify the validation data\n",
    "    history = model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), verbose=0)\n",
    "    histories[str(opt)] = history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6efce69-ebb2-44ed-9eb3-a051718eb7db",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Plot the training and validation accuracy and loss for each optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b84723-8673-4411-ae88-024e81fea744",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for opt, history in histories.items():\n",
    "    plt.plot(history.history['accuracy'], label=f'{opt} - Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label=f'{opt} - Validation Accuracy')\n",
    "plt.title('Model Accuracy by Optimizer')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for opt, history in histories.items():\n",
    "    plt.plot(history.history['loss'], label=f'{opt} - Loss')\n",
    "    plt.plot(history.history['val_loss'], label=f'{opt} - Validation Loss')\n",
    "plt.title('Model Loss by Optimizer')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ccce81-02f4-454d-92b1-407b051730c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Analysis and Questions\n",
    "* Which optimizer provided the fastest convergence?\n",
    "* Which optimizer achieved the highest accuracy on the validation set?\n",
    "* Discuss the possible reasons behind the performance differences observed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e970bb7-e441-46f2-9764-b57f8a5c2715",
   "metadata": {},
   "source": [
    "# Part 2: Regression task\n",
    "Now, let's apply the same set of optimizers to a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8903a493-2f05-444f-ab1c-e8d0fac4d1e4",
   "metadata": {},
   "source": [
    "## Setup for Regression\n",
    "Import libraries and prepare a regression dataset. For simplicity, let's use a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ccc21-6c09-43fb-a8fa-651587416c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data for regression\n",
    "X_reg, y_reg = make_regression(n_samples=500, n_features=2, noise=15, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff2ade-85b1-4378-9c7b-d7361d32c428",
   "metadata": {},
   "source": [
    "## Define the Neural Network Model for Regression\n",
    "Create a function to build a neural network model suitable for regression. Use 2 hidden layeres of 10 nodes each both using relu activations. Dont forget the output layer. For regression it is recommended to use MSE as loss and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eec899-b33e-483f-adf1-31a98733d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_regression(optimizer):\n",
    "    # -- Define the model yourself here following the instuctions above.\n",
    "    model = \n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd0b4f-7ea3-4786-80b1-bafb694af0ce",
   "metadata": {},
   "source": [
    "## Experiment with Different Optimizers for Regression\n",
    "Repeat the same process as in the classification task, but now for regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043ae90-8b6f-4455-b2fd-c5e07c9daa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict_reg = {}\n",
    "\n",
    "for opt in optimizers:\n",
    "    print(f\"Training (Regression) with optimizer: {opt}\")\n",
    "    if opt==\"SGD_momentum\": \n",
    "        model = build_model_regression(tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9))\n",
    "    else:\n",
    "        model = build_model_regression(opt)\n",
    "    history = model.fit(X_train_reg, y_train_reg, epochs=100, validation_split=0.2, verbose=0)\n",
    "    history_dict_reg[opt] = history\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_mse = model.evaluate(X_test_reg, y_test_reg, verbose=0)\n",
    "    print(f\"Test MSE with {opt}: {test_mse:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c8024-0fbb-47c6-959d-b1d703c50231",
   "metadata": {},
   "source": [
    "## Visualization of Results for Regression\n",
    "Plot the training and validation loss (MSE) for each optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b852e6-983b-4b29-896b-a8e5fce2c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for opt, history in history_dict_reg.items():\n",
    "    plt.plot(history.history['mse'], label=f'{opt} - Train (Reg)')\n",
    "    plt.plot(history.history['val_mse'], label=f'{opt} - Val (Reg)')\n",
    "\n",
    "plt.title('Model MSE with Different Optimizers (Regression)')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823ca69e-96ae-47fa-a067-7c99dac457ad",
   "metadata": {},
   "source": [
    "## Analysis and Questions\n",
    "* Compare the performance of the optimizers between the classification and regression tasks.\n",
    "* Did certain optimizers perform better on one task than the other? Why might this be?\n",
    "* Discuss the implications of these findings for selecting optimizers in real-world applications.\n",
    "\n",
    "## Conclusion\n",
    "Reflect on the importance of understanding the strengths and limitations of different optimizers in relation to the specific nature of the problem and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0288ad-a1cf-4be6-9b68-a74ddcdf821b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e657b-072f-4f17-9d0d-224b55867a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104dc1a7-6cbe-4b76-82a2-74d647ea6a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
